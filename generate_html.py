import os
import sys
import textwrap
try:
    from PIL import Image
except ImportError:
    print("éŒ¯èª¤: éœ€è¦ Pillow å¥—ä»¶ã€‚è«‹åŸ·è¡Œ: pip install Pillow")
    sys.exit(1)

# --- ä½¿ç”¨è€…è¨­å®šå€ (Hardcoded Dates) ---
ALL_DATES = [
    '20251122',
    '20251123',
    '20251124',
    '20251125',
    '20251126',
    '20251127',
    '20251128',
    '20251129',
    '20251130',
]

# --- è·¯å¾‘è¨­å®š ---
OUTPUT_HTML_IMG_PATH = 'photos_compressed/' # HTML è£¡é¢çš„ srcè·¯å¾‘
LOCAL_IMG_FOLDER = 'photos_compressed/'     # æœ¬æ©Ÿè®€å–åœ–ç‰‡å°ºå¯¸çš„è·¯å¾‘
YOUTUBE_ID_FILE = 'youtube_id.txt'
CSS_FILE = 'style.css'
INDEX_CONFIG_FILE = 'index.txt'
SUMMARY_CONFIG_FILE = 'summary.txt'

# --- è¼”åŠ©å‡½å¼ ---

def load_youtube_ids(filename):
    mapping = {}
    if not os.path.exists(filename):
        return mapping
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 2:
                fname = parts[0]
                url_or_id = parts[1]
                if 'youtu.be' in url_or_id or 'youtube.com' in url_or_id:
                    if '=' in url_or_id: vid_id = url_or_id.split('=')[-1]
                    else: vid_id = url_or_id.split('/')[-1]
                else: vid_id = url_or_id
                mapping[fname] = vid_id
    return mapping

def get_image_dimensions(filename):
    path = os.path.join(LOCAL_IMG_FOLDER, filename)
    if not os.path.exists(path):
        return None, None
    try:
        with Image.open(path) as img:
            return img.width, img.height
    except:
        return None, None

def get_date_display(date_str):
    if len(date_str) != 8: return date_str
    return f"{date_str[:4]} å¹´ {date_str[4:6]} æœˆ {date_str[6:]} æ—¥"

def get_formatted_date(date_str):
    if len(date_str) != 8: return date_str
    return f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:]}"

def get_js_content():
    return textwrap.dedent("""
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const navLinks = document.querySelectorAll('.sub-nav-inner a');
        if(navLinks.length === 0) return;

        const sections = document.querySelectorAll('.section-anchor');
        let isClicking = false;

        navLinks.forEach(link => {
            link.addEventListener('click', function(e) {
                navLinks.forEach(l => l.classList.remove('active'));
                this.classList.add('active');
                isClicking = true;
                setTimeout(() => { isClicking = false; }, 800); 
                this.scrollIntoView({ behavior: 'smooth', inline: 'center', block: 'nearest' });
            });
        });

        window.addEventListener('scroll', function() {
            if (isClicking) return;
            let current = '';
            const triggerLine = window.scrollY + 150; 
            sections.forEach(section => {
                if (triggerLine >= section.offsetTop) {
                    current = section.getAttribute('id');
                }
            });
            if ((window.innerHeight + window.scrollY) >= document.body.offsetHeight - 50) {
                if (sections.length > 0) current = sections[sections.length - 1].getAttribute('id');
            }
            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('active');
                    link.scrollIntoView({ behavior: 'smooth', inline: 'center', block: 'nearest' });
                }
            });
        });
    });
    </script>
    """).strip()

def get_navbar_html(current_page_key):
    links = []
    cls = ' class="active"' if current_page_key == 'home' else ''
    links.append(f'<a href="index.html"{cls}>é¦–é </a>')
    for i, date_str in enumerate(ALL_DATES):
        cls = ' class="active"' if current_page_key == date_str else ''
        links.append(f'<a href="{date_str}.html"{cls}>Day {i+1}</a>')
    cls = ' class="active"' if current_page_key == 'summary' else ''
    links.append(f'<a href="summary.html"{cls}>ç¸½çµ</a>')
    return f'<nav class="main-nav"><div class="nav-inner">{"".join(links)}</div></nav>'

# --- è§£æé‚è¼¯ (Parsing Logic) ---

def parse_p_block(buffer):
    """
    è§£æ p ... end p å€å¡Š
    ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œï¼Œå…¶é¤˜æ˜¯å…§å®¹
    """
    if not buffer:
        return None, None
    title = buffer[0]
    content = "<br>".join(buffer[1:]) if len(buffer) > 1 else ""
    return title, content

def parse_index_txt():
    """
    è®€å– index.txt
    å›å‚³: (main_title, subtitle, cover_map, journal_blocks)
    """
    default_title = "æˆ‘çš„æ—…éŠæ—¥èªŒ"
    default_subtitle = "æ”¶è—ç¾å¥½çš„æ™‚å…‰èˆ‡å›æ†¶"
    cover_map = {}
    journal_blocks = []

    if not os.path.exists(INDEX_CONFIG_FILE):
        return default_title, default_subtitle, cover_map, journal_blocks

    with open(INDEX_CONFIG_FILE, 'r', encoding='utf-8') as f:
        lines = [l.rstrip() for l in f.readlines()] # ä¿ç•™ç©ºç™½è¡Œçµæ§‹ä½†å»é™¤å°¾ç«¯æ›è¡Œ

    # è®€å–æ¨™é¡Œ (Line 1) å’Œ å‰¯æ¨™é¡Œ (Line 2)
    # éæ¿¾æ‰é–‹é ­çš„ç©ºè¡Œ
    content_lines = [l for l in lines if l]
    main_title = content_lines[0] if len(content_lines) > 0 else default_title
    subtitle = content_lines[1] if len(content_lines) > 1 else default_subtitle

    # ç‹€æ…‹æ©Ÿè§£æ
    in_cover = False
    in_p = False
    p_buffer = []

    for line in lines:
        stripped = line.strip()
        
        # ç‹€æ…‹åˆ‡æ›: front cover
        if stripped == 'front cover':
            in_cover = True
            continue
        elif stripped == 'end front cover':
            in_cover = False
            continue
        
        # ç‹€æ…‹åˆ‡æ›: p
        if stripped == 'p':
            in_p = True
            p_buffer = []
            continue
        elif stripped == 'end p':
            in_p = False
            title, content = parse_p_block(p_buffer)
            if title:
                journal_blocks.append({'title': title, 'content': content})
            continue

        # è™•ç†å…§å®¹
        if in_cover and stripped:
            # æ ¼å¼: DATE FILENAME (e.g., 20251122 img.jpg)
            parts = stripped.split(maxsplit=1)
            if len(parts) == 2:
                cover_map[parts[0]] = parts[1]
        
        elif in_p:
            # p å€å¡Šå…§ä¿ç•™åŸæ¨£ (åŒ…å«ç©ºå­—ä¸²ï¼Œè½‰æˆæ›è¡Œ)
            p_buffer.append(line)

    return main_title, subtitle, cover_map, journal_blocks

def parse_summary_txt():
    """
    è®€å– summary.txt
    å›å‚³: (title, subtitle, journal_blocks)
    """
    default_title = "æ—…ç¨‹ç¸½çµ"
    default_subtitle = ""
    journal_blocks = []

    if not os.path.exists(SUMMARY_CONFIG_FILE):
        return default_title, default_subtitle, journal_blocks

    with open(SUMMARY_CONFIG_FILE, 'r', encoding='utf-8') as f:
        lines = [l.rstrip() for l in f.readlines()]

    content_lines = [l for l in lines if l]
    title = content_lines[0] if len(content_lines) > 0 else default_title
    subtitle = content_lines[1] if len(content_lines) > 1 else default_subtitle

    in_p = False
    p_buffer = []

    for line in lines:
        stripped = line.strip()
        if stripped == 'p':
            in_p = True
            p_buffer = []
            continue
        elif stripped == 'end p':
            in_p = False
            j_title, j_content = parse_p_block(p_buffer)
            if j_title:
                journal_blocks.append({'title': j_title, 'content': j_content})
            continue
        
        if in_p:
            p_buffer.append(line)

    return title, subtitle, journal_blocks

def parse_date_txt(date_str):
    """
    è®€å– {date}.txt
    æ ¼å¼æ›´æ–°: text/end text -> p/end p (ç¬¬ä¸€è¡Œç‚ºæ¨™é¡Œ)
    """
    filename = f"{date_str}.txt"
    if not os.path.exists(filename):
        print(f"æç¤º: æ‰¾ä¸åˆ° {filename}ï¼Œå°‡ç•¥éæ­¤æ—¥æœŸå…§å®¹ã€‚")
        return [], 0, None

    blocks = []
    media_count = 0
    first_image = None
    
    in_section = False
    in_p = False
    
    section_buffer = []
    p_buffer = []
    section_counter = 0

    with open(filename, 'r', encoding='utf-8') as f:
        lines = [l.rstrip() for l in f.readlines()]

    for line in lines:
        stripped = line.strip()
        
        # 1. Section
        if stripped == 'section':
            in_section = True
            section_buffer = []
            continue
        elif stripped == 'end section':
            in_section = False
            if section_buffer:
                title = " ".join(section_buffer)
                blocks.append({
                    'type': 'section',
                    'title': title,
                    'id': f'sec-{section_counter}'
                })
                section_counter += 1
            continue
        
        if in_section:
            if stripped: section_buffer.append(stripped)
            continue

        # 2. p (Journal) - Updated Logic
        if stripped == 'p':
            in_p = True
            p_buffer = []
            continue
        elif stripped == 'end p':
            in_p = False
            title, content = parse_p_block(p_buffer)
            if title:
                blocks.append({
                    'type': 'journal',
                    'title': title,
                    'content': content
                })
            continue
        
        if in_p:
            p_buffer.append(line) # ä¿ç•™åŸå§‹è¡Œå…§å®¹ (å«ç©ºç™½)
            continue

        # 3. Media
        if not stripped:
            continue

        # --- [ä¿®æ­£] æ”¯æ´æª”åä¸­æœ‰ç©ºæ ¼ ---
        valid_exts = ['.jpg', '.jpeg', '.png', '.mp4', '.mov']
        lower_line = stripped.lower()
        
        split_pos = -1
        
        # ç­–ç•¥ A: æ‰¾å°‹ "å‰¯æª”å + ç©ºæ ¼" (ä»£è¡¨å¾Œé¢æœ‰åœ–èªª)
        # æˆ‘å€‘è¦æ‰¾"æœ€æ—©"å‡ºç¾çš„é‚£å€‹ï¼Œä»¥å…åœ–èªªè£¡ä¹Ÿæœ‰å‰¯æª”å
        min_idx = len(stripped) + 1
        found_len = 0
        
        for ext in valid_exts:
            search_str = ext + " "
            idx = lower_line.find(search_str)
            if idx != -1 and idx < min_idx:
                min_idx = idx
                found_len = len(ext)
        
        if min_idx != len(stripped) + 1:
            split_pos = min_idx + found_len
        else:
            # ç­–ç•¥ B: å¦‚æœæ²’åœ–èªªï¼Œæª¢æŸ¥æ˜¯å¦ç›´æ¥ä»¥å‰¯æª”åçµå°¾
            for ext in valid_exts:
                if lower_line.endswith(ext):
                    split_pos = len(stripped)
                    break
        
        # å¦‚æœæˆåŠŸè¾¨è­˜å‡ºæª”å
        if split_pos != -1:
            fname = stripped[:split_pos]
            description = stripped[split_pos:].strip()
            
            is_video = fname.lower().endswith(('.mp4', '.mov'))
            blocks.append({
                'type': 'media',
                'filename': fname,
                'caption': description,
                'is_video': is_video
            })
            media_count += 1
            if not is_video and first_image is None:
                first_image = fname

    return blocks, media_count, first_image

# --- é é¢ç”Ÿæˆå‡½å¼ ---

def create_date_html(date_str, blocks, youtube_map, main_site_title):
    display_date = get_date_display(date_str)
    day_idx = ALL_DATES.index(date_str) + 1
    page_title = f"{main_site_title} Day {day_idx}" # Head Title
    
    # 1. Sub-Navbar
    sub_nav_links = []
    for b in blocks:
        if b['type'] == 'section':
            sub_nav_links.append(f'<a href="#{b["id"]}">{b["title"]}</a>')
    
    sub_navbar_html = ""
    if sub_nav_links:
        sub_navbar_html = f'<nav class="sub-nav"><div class="sub-nav-inner">{"".join(sub_nav_links)}</div></nav>'

    # 2. Content
    content_lines = []
    content_lines.append(f'<div class="page-header"><h1>Day {day_idx}</h1><p>{display_date}</p></div>')
    content_lines.append('<div class="timeline-container">')

    for b in blocks:
        if b['type'] == 'section':
            content_lines.append(f'\n    <div id="{b["id"]}" class="section-anchor"></div>')
            content_lines.append(f'    <div class="section-header"><span class="section-dot"></span><h2>{b["title"]}</h2></div>\n')
        
        elif b['type'] == 'journal':
            # æ–°çš„ Journal çµæ§‹ï¼šæ¨™é¡Œ + å…§å®¹
            content_lines.append(textwrap.dedent(f"""
            <div class="journal-block">
                <h3>{b['title']}</h3>
                <p>{b['content']}</p>
            </div>"""))
        
        elif b['type'] == 'media':
            fname = b['filename']
            caption_text = b['caption']
            caption_extra = ""
            media_html = ""

            if b['is_video']:
                yt_id = youtube_map.get(fname)
                if yt_id:
                    media_html = (
                        f'<iframe width="100%" height="100%" '
                        f'src="https://www.youtube.com/embed/{yt_id}?rel=0" '
                        f'title="YouTube video player" frameborder="0" '
                        f'allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" '
                        f'referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>'
                    )
                    caption_extra = "(YouTube å½±ç‰‡)"
                else:
                    media_html = f'<div style="padding:40px;background:#eee;text-align:center;color:#666;">å½±ç‰‡ {fname} å°šæœªè¨­å®š YouTube ID</div>'
                    caption_extra = "(å½±ç‰‡å°šæœªé€£çµ)"
            else:
                img_attr = ' loading="lazy"'
                w, h = get_image_dimensions(fname)
                if w and h:
                    img_attr = f' width="{w}" height="{h}" loading="lazy" style="aspect-ratio:{w}/{h};"'
                media_html = f'<img src="{OUTPUT_HTML_IMG_PATH}{fname}"{img_attr}>'

            final_caption = caption_text if caption_text else f"é€™æ˜¯ {fname} çš„åœ–èªª... "
            if caption_extra:
                final_caption += f" {caption_extra}"

            content_lines.append(textwrap.dedent(f"""
            <article class="media-item">
                <div class="media-content">{media_html}</div>
                <div class="caption">
                    <div>{final_caption}</div>
                    <div class="filename-ref">{fname}</div>
                </div>
            </article>"""))

    content_lines.append('</div>')

    # Pagination
    idx = ALL_DATES.index(date_str)
    prev_link = f'{ALL_DATES[idx-1]}.html' if idx > 0 else 'index.html'
    prev_text = 'â† å‰ä¸€å¤©' if idx > 0 else 'â† å›é¦–é '
    next_link = f'{ALL_DATES[idx+1]}.html' if idx < len(ALL_DATES) - 1 else 'summary.html'
    next_text = 'ä¸‹ä¸€å¤© â†’' if idx < len(ALL_DATES) - 1 else 'çœ‹ç¸½çµ â†’'
    
    content_lines.append(f'\n<div class="pagination"><a href="{prev_link}" class="btn">{prev_text}</a><a href="{next_link}" class="btn">{next_text}</a></div>')

    full_html = textwrap.dedent(f"""
    <!DOCTYPE html>
    <html lang="zh-TW">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{page_title}</title>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@300;400;500;700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="{CSS_FILE}">
    </head>
    <body>
    {get_navbar_html(date_str)}
    {sub_navbar_html}
    <main>
    {chr(10).join(content_lines)}
    </main>
    {get_js_content()}
    </body>
    </html>
    """).strip()

    with open(f"{date_str}.html", 'w', encoding='utf-8') as f:
        f.write(full_html)
    print(f"å·²ç”Ÿæˆ: {date_str}.html (Title: {page_title})")


def create_index_html(day_infos, main_title, subtitle, journal_blocks, cover_map):
    """
    ç”Ÿæˆ index.html
    day_infos: list of dict {'date': '...', 'count': 123, 'cover': '...'}
    """
    content_lines = []
    content_lines.append(f'<div class="page-header"><h1>{main_title}</h1><p>{subtitle}</p></div>')
    content_lines.append('<div class="home-grid">')

    for info in day_infos:
        date_str = info['date']
        count = info['count']
        
        # æ±ºå®šå°é¢åœ–ï¼šå„ªå…ˆæŸ¥ index.txt çš„è¨­å®šï¼Œæ²’æœ‰å‰‡ç”¨ç•¶å¤©ç¬¬ä¸€å¼µ
        cover_img = cover_map.get(date_str, info['cover'])
        
        img_html = '<div class="placeholder-gradient"></div>'
        if cover_img:
            img_attr = ' loading="lazy"'
            w, h = get_image_dimensions(cover_img)
            if w and h:
                img_attr = f' width="{w}" height="{h}" style="aspect-ratio:{w}/{h};" loading="lazy"'
            img_html = f'<img src="{OUTPUT_HTML_IMG_PATH}{cover_img}"{img_attr}>'
        
        card_html = textwrap.dedent(f"""
        <a href="{date_str}.html" class="day-card">
            <div class="card-img-wrap">{img_html}</div>
            <div class="card-content"><h3>Day {ALL_DATES.index(date_str)+1}</h3><p>{get_formatted_date(date_str)} â€¢ {count} å€‹é …ç›®</p></div>
        </a>""")
        content_lines.append(card_html)

    # ç¸½çµå¡ç‰‡ (ä¹Ÿæª¢æŸ¥æ˜¯å¦æœ‰è‡ªè¨‚å°é¢)
    summary_cover = cover_map.get('summary', '')
    summary_img_html = '<div style="width:100%;height:100%;background:#4a5568;"></div>' # é è¨­ç°åº•
    if summary_cover:
        w, h = get_image_dimensions(summary_cover)
        img_attr = ' loading="lazy"'
        if w and h:
            img_attr = f' width="{w}" height="{h}" style="aspect-ratio:{w}/{h};" loading="lazy"'
        summary_img_html = f'<img src="{OUTPUT_HTML_IMG_PATH}{summary_cover}"{img_attr}>'

    content_lines.append(textwrap.dedent(f"""
    <a href="summary.html" class="day-card">
        <div class="card-img-wrap">{summary_img_html}</div>
        <div class="card-content"><h3>æ—…ç¨‹ç¸½çµ</h3><p>å¿ƒå¾—ã€å¾Œè¨˜èˆ‡ç²¾é¸å›æ†¶</p></div>
    </a>
    """).strip())
    
    content_lines.append('</div>')

    # åŠ å…¥ index.txt è£¡çš„ Journal Blocks
    if journal_blocks:
        content_lines.append('\n')
        for b in journal_blocks:
            content_lines.append(textwrap.dedent(f"""
            <div class="journal-block">
                <h3>{b['title']}</h3>
                <p>{b['content']}</p>
            </div>"""))

    full_html = textwrap.dedent(f"""
    <!DOCTYPE html>
    <html lang="zh-TW">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{main_title}</title>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@300;400;500;700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="{CSS_FILE}">
    </head>
    <body>
    {get_navbar_html('home')}

    <main>
    {chr(10).join(content_lines)}
    </main>
    {get_js_content()}
    </body>
    </html>
    """).strip()

    with open("index.html", 'w', encoding='utf-8') as f:
        f.write(full_html)
    print(f"å·²ç”Ÿæˆ: index.html (Title: {main_title})")

def create_summary_html(main_site_title, page_title, subtitle, journal_blocks):
    last_date_link = f"{ALL_DATES[-1]}.html" if ALL_DATES else "index.html"
    
    content_lines = []
    content_lines.append(f'<div class="page-header"><h1>{page_title}</h1><p>{subtitle}</p></div>')
    
    for b in journal_blocks:
        content_lines.append(textwrap.dedent(f"""
        <div class="journal-block">
            <h3>{b['title']}</h3>
            <p>{b['content']}</p>
        </div>"""))
        
    content_lines.append(f'<div class="pagination"><a href="{last_date_link}" class="btn">â† å›åˆ°æœ€å¾Œä¸€å¤©</a><a href="index.html" class="btn">å›é¦–é  ğŸ </a></div>')

    full_html = textwrap.dedent(f"""
    <!DOCTYPE html>
    <html lang="zh-TW">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>{main_site_title} summary</title>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@300;400;500;700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="{CSS_FILE}">
    </head>
    <body>
    {get_navbar_html('summary')}

    <main>
    {chr(10).join(content_lines)}
    </main>
    {get_js_content()}
    </body>
    </html>
    """).strip()

    with open("summary.html", 'w', encoding='utf-8') as f:
        f.write(full_html)
    print("å·²ç”Ÿæˆ: summary.html")

def main():
    print("--- é–‹å§‹å»ºç½®æ‰€æœ‰ç¶²é  ---")
    
    # 1. è®€å–è¨­å®šæª” (Index & Summary)
    site_title, site_subtitle, cover_map, index_journals = parse_index_txt()
    summary_title, summary_subtitle, summary_journals = parse_summary_txt()
    youtube_map = load_youtube_ids(YOUTUBE_ID_FILE)
    
    day_infos = [] # å„²å­˜æ¯ä¸€å¤©çš„çµ±è¨ˆè³‡è¨Šçµ¦ index ç”¨

    # 2. ç”Ÿæˆæ¯ä¸€å¤©çš„å…§é 
    for date_str in ALL_DATES:
        blocks, count, first_img = parse_date_txt(date_str)
        
        # ç”¢ç”Ÿè©²æ—¥æœŸçš„ HTML
        create_date_html(date_str, blocks, youtube_map, site_title)
        
        day_infos.append({
            'date': date_str,
            'count': count,
            'cover': first_img # é€™æ˜¯å‚™æ¡ˆï¼Œå¦‚æœ index.txt æ²’æŒ‡å®šå°é¢å°±æœƒç”¨é€™å€‹
        })

    # 3. ç”Ÿæˆ index.html
    create_index_html(day_infos, site_title, site_subtitle, index_journals, cover_map)
    
    # 4. ç”Ÿæˆ summary.html
    create_summary_html(site_title, summary_title, summary_subtitle, summary_journals)
    
    print("--- å…¨éƒ¨å®Œæˆ ---")

if __name__ == "__main__":
    main()